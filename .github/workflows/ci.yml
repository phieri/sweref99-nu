name: Build all

on:
  push:
    branches:
      - main
    paths-ignore:
      - '**.md'
  pull_request:
    branches:
      - main
    paths-ignore:
      - '**.md'

jobs:
  build-sqlite:
    name: Build SQLite
    runs-on: ubuntu-latest
    steps:
      - name: Cache SQLite source
        id: cache-sqlite-source
        uses: actions/cache@v4
        with:
          path: sqlite-amalgamation-3420000.zip
          key: sqlite-3420000-source
      - name: Download SQLite source
        if: steps.cache-sqlite-source.outputs.cache-hit != 'true'
        run: |
          wget https://www.sqlite.org/2023/sqlite-amalgamation-3420000.zip
      - name: Cache SQLite build
        id: cache-sqlite-build
        uses: actions/cache@v4
        with:
          path: |
            sqlite-amalgamation-3420000/libsqlite3.a
            sqlite-amalgamation-3420000/sqlite3.h
          key: sqlite-3420000-build-${{ runner.os }}-emcc
      - name: Install build dependencies
        if: steps.cache-sqlite-build.outputs.cache-hit != 'true'
        run: |
          sudo apt-get -qq update && sudo apt-get --quiet --assume-yes install \
            emscripten build-essential
      - name: Build SQLite
        if: steps.cache-sqlite-build.outputs.cache-hit != 'true'
        run: |
          unzip sqlite-amalgamation-3420000.zip
          cd sqlite-amalgamation-3420000
          emcc -Os -c sqlite3.c -o sqlite3.o
          emar rcs libsqlite3.a sqlite3.o
      - uses: actions/upload-artifact@v4
        with:
          name: sqlite3
          path: |
            sqlite-amalgamation-3420000/libsqlite3.a
            sqlite-amalgamation-3420000/sqlite3.h
  build-proj:
    name: Build PROJ
    runs-on: ubuntu-latest
    needs: build-sqlite
    steps:
      - uses: actions/checkout@v5
        with:
          repository: OSGeo/PROJ
          ref: 9.6.2
      - uses: actions/download-artifact@v4
        with:
          name: sqlite3
          path: ./sqlite3/
      - name: Cache PROJ build
        id: cache-proj-build
        uses: actions/cache@v4
        with:
          path: |
            build/install/lib/libproj.a
            build/install/include
          key: proj-9.6.2-optimized-build-${{ runner.os }}-emcc-${{ hashFiles('sqlite3/libsqlite3.a') }}-v3
      - name: Install build dependencies
        if: steps.cache-proj-build.outputs.cache-hit != 'true'
        run: |
          sudo apt-get -qq update && sudo apt-get --quiet --assume-yes install \
            emscripten build-essential cmake python3
      - name: Create optimization script
        if: steps.cache-proj-build.outputs.cache-hit != 'true'
        run: |
          cat > optimize_proj_db.py << 'EOF'
          #!/usr/bin/env python3
          """
          Optimize proj.db by removing unwanted coordinate systems using a subtractive approach.
          This script identifies Swedish coordinate systems and removes all others.
          """
          
          import sqlite3
          import sys
          import os
          
          def get_swedish_coordinate_systems():
              """Define the coordinate systems needed for Swedish applications"""
              return {
                  'geodetic_crs': ['4326', '4619'],  # WGS84, SWEREF99
                  'projected_crs': ['3006'],         # SWEREF99 TM
                  'conversion': ['17333'],           # SWEREF99 TM conversion
                  'geodetic_datum': [],
                  'ellipsoid': [],
                  'prime_meridian': [],
                  'unit_of_measure': [],
                  'celestial_body': [],
                  'coordinate_system': [],
                  'extent': [],
                  'scope': [],
                  'usage': []
              }
          
          def analyze_dependencies(db_path):
              """Analyze what entries are needed based on Swedish coordinate systems"""
              conn = sqlite3.connect(db_path)
              needed_ids = get_swedish_coordinate_systems()
              
              # Iteratively find dependencies
              iterations = 0
              while iterations < 10:  # Prevent infinite loops
                  prev_total = sum(len(ids) for ids in needed_ids.values())
                  
                  # Find dependencies for geodetic_crs
                  for crs_code in needed_ids['geodetic_crs']:
                      cursor = conn.execute("""
                          SELECT datum_auth_name, datum_code, 
                                 coordinate_system_auth_name, coordinate_system_code
                          FROM geodetic_crs 
                          WHERE auth_name='EPSG' AND code=?
                      """, (crs_code,))
                      for row in cursor:
                          if row[0] and row[1]:  # datum
                              if row[1] not in needed_ids['geodetic_datum']:
                                  needed_ids['geodetic_datum'].append(row[1])
                          if row[2] and row[3]:  # coordinate system
                              if row[3] not in needed_ids['coordinate_system']:
                                  needed_ids['coordinate_system'].append(row[3])
                  
                  # Find dependencies for projected_crs  
                  for crs_code in needed_ids['projected_crs']:
                      cursor = conn.execute("""
                          SELECT geodetic_crs_auth_name, geodetic_crs_code,
                                 coordinate_system_auth_name, coordinate_system_code,
                                 conversion_auth_name, conversion_code
                          FROM projected_crs 
                          WHERE auth_name='EPSG' AND code=?
                      """, (crs_code,))
                      for row in cursor:
                          if row[0] and row[1]:  # geodetic_crs
                              if row[1] not in needed_ids['geodetic_crs']:
                                  needed_ids['geodetic_crs'].append(row[1])
                          if row[2] and row[3]:  # coordinate_system
                              if row[3] not in needed_ids['coordinate_system']:
                                  needed_ids['coordinate_system'].append(row[3])
                          if row[4] and row[5]:  # conversion
                              if row[5] not in needed_ids['conversion']:
                                  needed_ids['conversion'].append(row[5])
                  
                  # Find dependencies for geodetic_datum
                  for datum_code in needed_ids['geodetic_datum']:
                      cursor = conn.execute("""
                          SELECT ellipsoid_auth_name, ellipsoid_code,
                                 prime_meridian_auth_name, prime_meridian_code
                          FROM geodetic_datum 
                          WHERE auth_name='EPSG' AND code=?
                      """, (datum_code,))
                      for row in cursor:
                          if row[0] and row[1]:  # ellipsoid
                              if row[1] not in needed_ids['ellipsoid']:
                                  needed_ids['ellipsoid'].append(row[1])
                          if row[2] and row[3]:  # prime_meridian
                              if row[3] not in needed_ids['prime_meridian']:
                                  needed_ids['prime_meridian'].append(row[3])
                  
                  # Find dependencies for ellipsoid
                  for ellipsoid_code in needed_ids['ellipsoid']:
                      cursor = conn.execute("""
                          SELECT celestial_body_auth_name, celestial_body_code,
                                 uom_auth_name, uom_code
                          FROM ellipsoid 
                          WHERE auth_name='EPSG' AND code=?
                      """, (ellipsoid_code,))
                      for row in cursor:
                          if row[0] and row[1]:  # celestial_body
                              celestial_key = f"{row[0]}:{row[1]}"
                              if celestial_key not in needed_ids['celestial_body']:
                                  needed_ids['celestial_body'].append(celestial_key)
                          if row[2] and row[3]:  # unit_of_measure
                              if row[3] not in needed_ids['unit_of_measure']:
                                  needed_ids['unit_of_measure'].append(row[3])
                  
                  # Find dependencies for prime_meridian
                  for pm_code in needed_ids['prime_meridian']:
                      cursor = conn.execute("""
                          SELECT uom_auth_name, uom_code
                          FROM prime_meridian 
                          WHERE auth_name='EPSG' AND code=?
                      """, (pm_code,))
                      for row in cursor:
                          if row[0] and row[1]:  # unit_of_measure
                              if row[1] not in needed_ids['unit_of_measure']:
                                  needed_ids['unit_of_measure'].append(row[1])
                  
                  # Find usage records
                  for table_name in ['geodetic_crs', 'projected_crs', 'conversion']:
                      for code in needed_ids[table_name]:
                          cursor = conn.execute("""
                              SELECT extent_auth_name, extent_code,
                                     scope_auth_name, scope_code
                              FROM usage 
                              WHERE object_table_name=? AND object_auth_name='EPSG' AND object_code=?
                          """, (table_name, code))
                          for row in cursor:
                              if row[0] and row[1]:  # extent
                                  if row[1] not in needed_ids['extent']:
                                      needed_ids['extent'].append(row[1])
                              if row[2] and row[3]:  # scope
                                  if row[3] not in needed_ids['scope']:
                                      needed_ids['scope'].append(row[3])
                  
                  # Check if we found new dependencies
                  new_total = sum(len(ids) for ids in needed_ids.values())
                  if new_total == prev_total:
                      break  # No new dependencies found
                  iterations += 1
              
              conn.close()
              return needed_ids
          
          def optimize_proj_db(input_path, output_path=None):
              """Remove unwanted entries from proj.db keeping only Swedish coordinate systems"""
              if output_path is None:
                  output_path = input_path
              
              # Analyze what we need to keep
              print("Analyzing dependencies for Swedish coordinate systems...")
              needed_ids = analyze_dependencies(input_path)
              
              # Print what we're keeping
              total_kept = 0
              for table_name, ids in needed_ids.items():
                  if ids:
                      print(f"Keeping {len(ids)} entries in {table_name}: {ids[:5]}{'...' if len(ids) > 5 else ''}")
                      total_kept += len(ids)
              
              print(f"Total entries to keep: {total_kept}")
              
              # Get original size
              original_size = os.path.getsize(input_path)
              print(f"Original database size: {original_size:,} bytes")
              
              # Create optimized database
              conn = sqlite3.connect(input_path)
              
              # Count original entries
              original_counts = {}
              for table_name in needed_ids.keys():
                  if table_name != 'celestial_body':  # Special handling for celestial_body
                      cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                      original_counts[table_name] = cursor.fetchone()[0]
                  else:
                      cursor = conn.execute("SELECT COUNT(*) FROM celestial_body")
                      original_counts[table_name] = cursor.fetchone()[0]
              
              # Remove unwanted entries
              removed_counts = {}
              for table_name, keep_ids in needed_ids.items():
                  if not keep_ids:
                      continue
                      
                  if table_name == 'celestial_body':
                      # Special handling for celestial_body (auth_name:code format)
                      auth_codes = []
                      for item in keep_ids:
                          if ':' in item:
                              auth, code = item.split(':', 1)
                              auth_codes.append((auth, code))
                      
                      if auth_codes:
                          placeholders = ','.join(['(?,?)'] * len(auth_codes))
                          flat_params = []
                          for auth, code in auth_codes:
                              flat_params.extend([auth, code])
                          
                          cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                          before_count = cursor.fetchone()[0]
                          
                          conn.execute(f"""
                              DELETE FROM {table_name} 
                              WHERE (auth_name, code) NOT IN (VALUES {placeholders})
                          """, flat_params)
                          
                          cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                          after_count = cursor.fetchone()[0]
                          removed_counts[table_name] = before_count - after_count
                  else:
                      # Regular tables with EPSG auth_name
                      cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                      before_count = cursor.fetchone()[0]
                      
                      if table_name == 'usage':
                          # For usage table, we need to keep entries that reference our objects
                          keep_usage_sql = """
                              DELETE FROM usage WHERE NOT (
                                  (object_table_name='geodetic_crs' AND object_auth_name='EPSG' AND object_code IN ({}))
                                  OR (object_table_name='projected_crs' AND object_auth_name='EPSG' AND object_code IN ({}))
                                  OR (object_table_name='conversion' AND object_auth_name='EPSG' AND object_code IN ({}))
                              )
                          """.format(
                              ','.join('?' * len(needed_ids['geodetic_crs'])),
                              ','.join('?' * len(needed_ids['projected_crs'])),
                              ','.join('?' * len(needed_ids['conversion']))
                          )
                          params = needed_ids['geodetic_crs'] + needed_ids['projected_crs'] + needed_ids['conversion']
                          conn.execute(keep_usage_sql, params)
                      else:
                          placeholders = ','.join(['?'] * len(keep_ids))
                          conn.execute(f"""
                              DELETE FROM {table_name} 
                              WHERE auth_name='EPSG' AND code NOT IN ({placeholders})
                          """, keep_ids)
                      
                      cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                      after_count = cursor.fetchone()[0]
                      removed_counts[table_name] = before_count - after_count
              
              conn.commit()
              conn.close()
              
              # Report results
              optimized_size = os.path.getsize(input_path)
              print(f"\nOptimization complete!")
              print(f"Optimized database size: {optimized_size:,} bytes")
              print(f"Size reduction: {original_size - optimized_size:,} bytes ({(1 - optimized_size/original_size)*100:.1f}%)")
              
              print(f"\nEntries removed by table:")
              total_removed = 0
              for table_name, removed_count in removed_counts.items():
                  if removed_count > 0:
                      original_count = original_counts.get(table_name, 0)
                      kept_count = original_count - removed_count
                      print(f"  {table_name}: {removed_count:,} removed, {kept_count} kept")
                      total_removed += removed_count
              
              print(f"Total entries removed: {total_removed:,}")
              
              return optimized_size
          
          if __name__ == '__main__':
              if len(sys.argv) < 2:
                  print("Usage: python3 optimize_proj_db.py <proj.db_path> [output_path]")
                  sys.exit(1)
              
              input_path = sys.argv[1]
              output_path = sys.argv[2] if len(sys.argv) > 2 else None
              
              if not os.path.exists(input_path):
                  print(f"Error: Database file not found: {input_path}")
                  sys.exit(1)
              
              optimize_proj_db(input_path, output_path)
          EOF
      - name: Build PROJ
        if: steps.cache-proj-build.outputs.cache-hit != 'true'
        run: |
          # Remove build directory if it exists from previous partial runs
          rm -rf build/proj_build
          mkdir build/proj_build
          cd build/proj_build
          emcmake cmake ../.. \
            -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_C_FLAGS="-Os" \
            -DCMAKE_CXX_FLAGS="-Os" \
            -DBUILD_APPS=OFF \
            -DBUILD_TESTING=OFF \
            -DBUILD_SHARED_LIBS=OFF \
            -DSQLite3_INCLUDE_DIR=$(pwd)/../../sqlite3 \
            -DSQLite3_LIBRARY=$(pwd)/../../sqlite3/libsqlite3.a \
            -DENABLE_CURL=OFF \
            -DENABLE_TIFF=OFF \
            -DUSE_EXTERNAL_GTEST=OFF \
            -DBUILD_PROJSYNC=OFF \
            -DCMAKE_INSTALL_PREFIX=$(pwd)/../install
          emmake make -j$(nproc)
          emmake make install
      - name: Optimize PROJ database
        if: steps.cache-proj-build.outputs.cache-hit != 'true'
        run: |
          # Find the proj.db in the installed PROJ build
          PROJ_DB_PATH=$(find build/install -name "proj.db" | head -1)
          if [ -z "$PROJ_DB_PATH" ]; then
            echo "Error: proj.db not found in PROJ installation"
            find build/install -name "*.db" || echo "No .db files found"
            exit 1
          fi
          echo "Found proj.db at: $PROJ_DB_PATH"
          echo "Original size: $(ls -lh "$PROJ_DB_PATH" | awk '{print $5}')"
          
          # Optimize the database using subtractive approach
          python3 optimize_proj_db.py "$PROJ_DB_PATH"
          echo "Optimized size: $(ls -lh "$PROJ_DB_PATH" | awk '{print $5}')"
      - name: Stage artifacts
        run: |
          # Copy files to expected artifact structure
          mkdir -p artifact-staging
          cp build/install/lib/libproj.a artifact-staging/
          cp -r build/install/include artifact-staging/
          # Include SQLite3 library that PROJ depends on
          cp sqlite3/libsqlite3.a artifact-staging/
      - uses: actions/upload-artifact@v4
        with:
          name: proj
          path: artifact-staging/
  build-wasm:
    name: Build Wasm
    runs-on: ubuntu-24.04
    needs: build-proj
    steps:
      - uses: actions/checkout@v5
      - uses: actions/download-artifact@v4
        with:
          name: proj
          path: ./
      - name: Install build dependencies
        run: |
          sudo apt-get -qq update && sudo apt-get --quiet --assume-yes install \
            emscripten build-essential
      - run: |
          echo "=== Checking downloaded artifacts ==="
          ls -la
          echo "=== Creating build structure ==="
          # Create build directory and move artifacts to expected locations
          mkdir -p build/include
          # Move libproj.a to expected location
          if [ -f libproj.a ]; then
            mv libproj.a build/
            echo "Moved libproj.a to build/"
          else
            echo "ERROR: libproj.a not found"
            exit 1
          fi
          # Move libsqlite3.a to expected location
          if [ -f libsqlite3.a ]; then
            mv libsqlite3.a build/
            echo "Moved libsqlite3.a to build/"
          else
            echo "ERROR: libsqlite3.a not found"
            exit 1
          fi
          # Move headers to expected location
          if [ -d include ]; then
            mv include/* build/include/
            echo "Moved headers to build/include/"
          else
            echo "ERROR: include directory not found"
            exit 1
          fi
          echo "=== Final build structure ==="
          ls -la build/
          ls -la build/include/
          echo "=== Building WASM ==="
          make sr9.wasm
      - uses: actions/upload-artifact@v4
        with:
          name: wasm
          path: |
            sr9.wasm
            sr9.js
  build:
    name: Build
    runs-on: ubuntu-24.04
    needs: build-wasm
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
      - name: Cache TypeScript
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-typescript-global
          restore-keys: |
            ${{ runner.os }}-typescript-
      - run: npm install -g typescript
      - uses: actions/download-artifact@v4
        with:
          name: wasm
          path: ./
      - run: |
          echo "=== Checking downloaded WASM artifacts ==="
          ls -la
          echo "=== Building TypeScript ==="
          make script.js
          echo "=== Copying WASM files to _site ==="
          cp sr9.wasm _site/
          cp sr9.js _site/
          echo "=== Final _site contents ==="
          ls -la _site/
          echo "=== Adding additional files ==="
          echo "${{ vars.adstxt }}" > _site/ads.txt
          curl -o _site/pico.min.css https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.min.css
          echo "=== Final _site contents ==="
          ls -la _site/
      - uses: actions/upload-pages-artifact@v4
  deploy:
    name: Deploy
    needs: build
    if: github.ref == 'refs/heads/main'
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
